{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "name": "Lottery.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a572315-970e-4743-b21d-d470c1eefac9",
        "outputId": "7ca70fe9-b672-4dd2-bb68-afcddc7e92eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch\n",
        "import numpy as np\n",
        "torch.manual_seed(42)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
      ],
      "id": "8a572315-970e-4743-b21d-d470c1eefac9",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ac721c5-22ed-4bea-9491-0f3d0fc6b46c"
      },
      "source": [
        "?FashionMNIST"
      ],
      "id": "8ac721c5-22ed-4bea-9491-0f3d0fc6b46c",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "937ebf3e-26ff-417a-911f-a2444bc9bee5"
      },
      "source": [
        "datadir=\"data\"\n",
        "train_data = FashionMNIST(root=datadir, train=True, download=True, transform=ToTensor())\n",
        "test_data = FashionMNIST(root=datadir, train=False, download=True, transform=ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data,batch_size=32)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1)"
      ],
      "id": "937ebf3e-26ff-417a-911f-a2444bc9bee5",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b3ab9cb-9feb-428f-b2a3-6481db1ae6ca"
      },
      "source": [
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n"
      ],
      "id": "8b3ab9cb-9feb-428f-b2a3-6481db1ae6ca",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9431474-dc89-4590-93dc-790d55a55405"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "        self.first = True\n",
        "        self.start_weights = {}\n",
        "        self.end_weights = {}\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.first:\n",
        "            self.first = False\n",
        "            self.log_start_weights()\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "    def log_start_weights(self):\n",
        "        self.start_weights = {\n",
        "            \"conv1\": self.conv1.weight.cpu().detach().numpy(),\n",
        "            \"conv2\": self.conv2.weight.cpu().detach().numpy(),\n",
        "            \"fc1\": self.fc1.weight.cpu().detach().numpy(),\n",
        "            \"fc2\": self.fc2.weight.cpu().detach().numpy(),\n",
        "        }\n",
        "\n",
        "    def log_end_weights(self):\n",
        "        self.end_weights = {\n",
        "            \"conv1\": self.conv1.weight.cpu().detach().numpy(),\n",
        "            \"conv2\": self.conv2.weight.cpu().detach().numpy(),\n",
        "            \"fc1\": self.fc1.weight.cpu().detach().numpy(),\n",
        "            \"fc2\": self.fc2.weight.cpu().detach().numpy(),\n",
        "        }\n",
        "\n",
        "def train( model, device, train_loader, optimizer, epoch, log_interval=10):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            \n",
        "    model.log_end_weights()\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "id": "f9431474-dc89-4590-93dc-790d55a55405",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEvBCKdzvvVE"
      },
      "source": [
        "LR=0.001\n",
        "GAMMA=0.7\n",
        "EPOCHS=2\n",
        "device=\"cuda\"\n",
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=LR)\n"
      ],
      "id": "sEvBCKdzvvVE",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdAwbBRATQz7"
      },
      "source": [
        ""
      ],
      "id": "tdAwbBRATQz7",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63e1473d-131a-4479-8a82-032509d2f9fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "321530d9-3f10-4386-ed83-d39cf71b7ffb"
      },
      "source": [
        "\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=GAMMA)\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), \"mnist_cnn.pt\")"
      ],
      "id": "63e1473d-131a-4479-8a82-032509d2f9fa",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.293868\n",
            "Train Epoch: 1 [320/60000 (1%)]\tLoss: 2.302377\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.288112\n",
            "Train Epoch: 1 [960/60000 (2%)]\tLoss: 2.283208\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.278955\n",
            "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 2.313639\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.288851\n",
            "Train Epoch: 1 [2240/60000 (4%)]\tLoss: 2.306035\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.301236\n",
            "Train Epoch: 1 [2880/60000 (5%)]\tLoss: 2.293371\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.287654\n",
            "Train Epoch: 1 [3520/60000 (6%)]\tLoss: 2.289837\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.303721\n",
            "Train Epoch: 1 [4160/60000 (7%)]\tLoss: 2.307404\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.288959\n",
            "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 2.276751\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.280452\n",
            "Train Epoch: 1 [5440/60000 (9%)]\tLoss: 2.279994\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.279576\n",
            "Train Epoch: 1 [6080/60000 (10%)]\tLoss: 2.288529\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.279494\n",
            "Train Epoch: 1 [6720/60000 (11%)]\tLoss: 2.264379\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.287421\n",
            "Train Epoch: 1 [7360/60000 (12%)]\tLoss: 2.278325\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.294166\n",
            "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 2.290209\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.285394\n",
            "Train Epoch: 1 [8640/60000 (14%)]\tLoss: 2.300064\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.283262\n",
            "Train Epoch: 1 [9280/60000 (15%)]\tLoss: 2.269284\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.296613\n",
            "Train Epoch: 1 [9920/60000 (17%)]\tLoss: 2.286445\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.289407\n",
            "Train Epoch: 1 [10560/60000 (18%)]\tLoss: 2.276816\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.275893\n",
            "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 2.283372\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.253890\n",
            "Train Epoch: 1 [11840/60000 (20%)]\tLoss: 2.272061\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.269777\n",
            "Train Epoch: 1 [12480/60000 (21%)]\tLoss: 2.261602\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.257244\n",
            "Train Epoch: 1 [13120/60000 (22%)]\tLoss: 2.275465\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.268265\n",
            "Train Epoch: 1 [13760/60000 (23%)]\tLoss: 2.290316\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.280821\n",
            "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 2.268966\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.272859\n",
            "Train Epoch: 1 [15040/60000 (25%)]\tLoss: 2.258190\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.263945\n",
            "Train Epoch: 1 [15680/60000 (26%)]\tLoss: 2.283543\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.274211\n",
            "Train Epoch: 1 [16320/60000 (27%)]\tLoss: 2.267259\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.241900\n",
            "Train Epoch: 1 [16960/60000 (28%)]\tLoss: 2.249618\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.251988\n",
            "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 2.241917\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.232957\n",
            "Train Epoch: 1 [18240/60000 (30%)]\tLoss: 2.244772\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.238849\n",
            "Train Epoch: 1 [18880/60000 (31%)]\tLoss: 2.231100\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.261192\n",
            "Train Epoch: 1 [19520/60000 (33%)]\tLoss: 2.226237\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 2.244528\n",
            "Train Epoch: 1 [20160/60000 (34%)]\tLoss: 2.246827\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.225282\n",
            "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 2.255433\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 2.250877\n",
            "Train Epoch: 1 [21440/60000 (36%)]\tLoss: 2.232370\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.217036\n",
            "Train Epoch: 1 [22080/60000 (37%)]\tLoss: 2.236955\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.213613\n",
            "Train Epoch: 1 [22720/60000 (38%)]\tLoss: 2.208014\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 2.240151\n",
            "Train Epoch: 1 [23360/60000 (39%)]\tLoss: 2.229331\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 2.246201\n",
            "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 2.208700\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 2.191648\n",
            "Train Epoch: 1 [24640/60000 (41%)]\tLoss: 2.185123\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 2.231012\n",
            "Train Epoch: 1 [25280/60000 (42%)]\tLoss: 2.202590\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.185681\n",
            "Train Epoch: 1 [25920/60000 (43%)]\tLoss: 2.225832\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 2.187869\n",
            "Train Epoch: 1 [26560/60000 (44%)]\tLoss: 2.188931\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 2.217753\n",
            "Train Epoch: 1 [27200/60000 (45%)]\tLoss: 2.201263\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 2.192907\n",
            "Train Epoch: 1 [27840/60000 (46%)]\tLoss: 2.190607\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 2.195917\n",
            "Train Epoch: 1 [28480/60000 (47%)]\tLoss: 2.192372\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 2.203392\n",
            "Train Epoch: 1 [29120/60000 (49%)]\tLoss: 2.178971\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 2.182053\n",
            "Train Epoch: 1 [29760/60000 (50%)]\tLoss: 2.196119\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 2.160934\n",
            "Train Epoch: 1 [30400/60000 (51%)]\tLoss: 2.183325\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 2.163305\n",
            "Train Epoch: 1 [31040/60000 (52%)]\tLoss: 2.157012\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 2.171551\n",
            "Train Epoch: 1 [31680/60000 (53%)]\tLoss: 2.205779\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.158101\n",
            "Train Epoch: 1 [32320/60000 (54%)]\tLoss: 2.162075\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 2.142701\n",
            "Train Epoch: 1 [32960/60000 (55%)]\tLoss: 2.169926\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 2.135406\n",
            "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 2.140597\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 2.149858\n",
            "Train Epoch: 1 [34240/60000 (57%)]\tLoss: 2.141756\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 2.120245\n",
            "Train Epoch: 1 [34880/60000 (58%)]\tLoss: 2.115846\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 2.131713\n",
            "Train Epoch: 1 [35520/60000 (59%)]\tLoss: 2.078630\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 2.119864\n",
            "Train Epoch: 1 [36160/60000 (60%)]\tLoss: 2.128704\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 2.122209\n",
            "Train Epoch: 1 [36800/60000 (61%)]\tLoss: 2.096141\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 2.106421\n",
            "Train Epoch: 1 [37440/60000 (62%)]\tLoss: 2.106371\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 2.122366\n",
            "Train Epoch: 1 [38080/60000 (63%)]\tLoss: 2.098813\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.106507\n",
            "Train Epoch: 1 [38720/60000 (65%)]\tLoss: 2.112577\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 2.072629\n",
            "Train Epoch: 1 [39360/60000 (66%)]\tLoss: 2.078915\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 2.129093\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 2.082161\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 2.091578\n",
            "Train Epoch: 1 [40640/60000 (68%)]\tLoss: 2.034890\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 2.095591\n",
            "Train Epoch: 1 [41280/60000 (69%)]\tLoss: 2.118839\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 2.071469\n",
            "Train Epoch: 1 [41920/60000 (70%)]\tLoss: 2.103131\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 2.061109\n",
            "Train Epoch: 1 [42560/60000 (71%)]\tLoss: 2.079043\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 2.058267\n",
            "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 2.059494\n",
            "Train Epoch: 1 [43520/60000 (73%)]\tLoss: 2.043268\n",
            "Train Epoch: 1 [43840/60000 (73%)]\tLoss: 2.069123\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 2.055115\n",
            "Train Epoch: 1 [44480/60000 (74%)]\tLoss: 2.020716\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.046403\n",
            "Train Epoch: 1 [45120/60000 (75%)]\tLoss: 2.043713\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 2.014229\n",
            "Train Epoch: 1 [45760/60000 (76%)]\tLoss: 1.986237\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 2.036285\n",
            "Train Epoch: 1 [46400/60000 (77%)]\tLoss: 1.971378\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 1.993750\n",
            "Train Epoch: 1 [47040/60000 (78%)]\tLoss: 1.981792\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 2.041241\n",
            "Train Epoch: 1 [47680/60000 (79%)]\tLoss: 1.975270\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.973747\n",
            "Train Epoch: 1 [48320/60000 (81%)]\tLoss: 1.970070\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 2.026742\n",
            "Train Epoch: 1 [48960/60000 (82%)]\tLoss: 2.004004\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 2.037034\n",
            "Train Epoch: 1 [49600/60000 (83%)]\tLoss: 1.923974\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 1.947546\n",
            "Train Epoch: 1 [50240/60000 (84%)]\tLoss: 1.980855\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 2.010359\n",
            "Train Epoch: 1 [50880/60000 (85%)]\tLoss: 1.937707\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.886475\n",
            "Train Epoch: 1 [51520/60000 (86%)]\tLoss: 1.948444\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 1.935000\n",
            "Train Epoch: 1 [52160/60000 (87%)]\tLoss: 1.937313\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 1.955520\n",
            "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 1.932270\n",
            "Train Epoch: 1 [53120/60000 (89%)]\tLoss: 1.922350\n",
            "Train Epoch: 1 [53440/60000 (89%)]\tLoss: 1.866681\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.916677\n",
            "Train Epoch: 1 [54080/60000 (90%)]\tLoss: 1.908514\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 2.023738\n",
            "Train Epoch: 1 [54720/60000 (91%)]\tLoss: 1.889002\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 1.862210\n",
            "Train Epoch: 1 [55360/60000 (92%)]\tLoss: 1.886121\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 1.862407\n",
            "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 1.881212\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 1.935470\n",
            "Train Epoch: 1 [56640/60000 (94%)]\tLoss: 1.862024\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 1.934834\n",
            "Train Epoch: 1 [57280/60000 (95%)]\tLoss: 1.849057\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.857803\n",
            "Train Epoch: 1 [57920/60000 (97%)]\tLoss: 1.836658\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 1.804506\n",
            "Train Epoch: 1 [58560/60000 (98%)]\tLoss: 1.820014\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 1.783213\n",
            "Train Epoch: 1 [59200/60000 (99%)]\tLoss: 1.790629\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 1.797736\n",
            "Train Epoch: 1 [59840/60000 (100%)]\tLoss: 1.808458\n",
            "\n",
            "Test set: Average loss: 1.7968, Accuracy: 6240/10000 (62%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.874755\n",
            "Train Epoch: 2 [320/60000 (1%)]\tLoss: 1.855785\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 1.744464\n",
            "Train Epoch: 2 [960/60000 (2%)]\tLoss: 1.857241\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 1.768039\n",
            "Train Epoch: 2 [1600/60000 (3%)]\tLoss: 1.828371\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 1.785802\n",
            "Train Epoch: 2 [2240/60000 (4%)]\tLoss: 1.805991\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 1.860539\n",
            "Train Epoch: 2 [2880/60000 (5%)]\tLoss: 1.797244\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 1.707785\n",
            "Train Epoch: 2 [3520/60000 (6%)]\tLoss: 1.848867\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 1.756526\n",
            "Train Epoch: 2 [4160/60000 (7%)]\tLoss: 1.726826\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 1.811044\n",
            "Train Epoch: 2 [4800/60000 (8%)]\tLoss: 1.737857\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 1.759751\n",
            "Train Epoch: 2 [5440/60000 (9%)]\tLoss: 1.778867\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 1.742359\n",
            "Train Epoch: 2 [6080/60000 (10%)]\tLoss: 1.847809\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.817592\n",
            "Train Epoch: 2 [6720/60000 (11%)]\tLoss: 1.776714\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 1.779054\n",
            "Train Epoch: 2 [7360/60000 (12%)]\tLoss: 1.803594\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 1.810111\n",
            "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 1.774073\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 1.744292\n",
            "Train Epoch: 2 [8640/60000 (14%)]\tLoss: 1.780782\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 1.816741\n",
            "Train Epoch: 2 [9280/60000 (15%)]\tLoss: 1.635267\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 1.783361\n",
            "Train Epoch: 2 [9920/60000 (17%)]\tLoss: 1.807237\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 1.683903\n",
            "Train Epoch: 2 [10560/60000 (18%)]\tLoss: 1.662085\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 1.675940\n",
            "Train Epoch: 2 [11200/60000 (19%)]\tLoss: 1.712691\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 1.725304\n",
            "Train Epoch: 2 [11840/60000 (20%)]\tLoss: 1.683845\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 1.746501\n",
            "Train Epoch: 2 [12480/60000 (21%)]\tLoss: 1.721124\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.717733\n",
            "Train Epoch: 2 [13120/60000 (22%)]\tLoss: 1.809510\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 1.726810\n",
            "Train Epoch: 2 [13760/60000 (23%)]\tLoss: 1.802270\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 1.772334\n",
            "Train Epoch: 2 [14400/60000 (24%)]\tLoss: 1.642123\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 1.727698\n",
            "Train Epoch: 2 [15040/60000 (25%)]\tLoss: 1.719678\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 1.655478\n",
            "Train Epoch: 2 [15680/60000 (26%)]\tLoss: 1.783697\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 1.807482\n",
            "Train Epoch: 2 [16320/60000 (27%)]\tLoss: 1.612781\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 1.616211\n",
            "Train Epoch: 2 [16960/60000 (28%)]\tLoss: 1.692140\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 1.678447\n",
            "Train Epoch: 2 [17600/60000 (29%)]\tLoss: 1.649110\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 1.554632\n",
            "Train Epoch: 2 [18240/60000 (30%)]\tLoss: 1.652360\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 1.769584\n",
            "Train Epoch: 2 [18880/60000 (31%)]\tLoss: 1.600919\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.616461\n",
            "Train Epoch: 2 [19520/60000 (33%)]\tLoss: 1.683538\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 1.710306\n",
            "Train Epoch: 2 [20160/60000 (34%)]\tLoss: 1.783440\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 1.506548\n",
            "Train Epoch: 2 [20800/60000 (35%)]\tLoss: 1.558394\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 1.541723\n",
            "Train Epoch: 2 [21440/60000 (36%)]\tLoss: 1.718652\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 1.488684\n",
            "Train Epoch: 2 [22080/60000 (37%)]\tLoss: 1.730445\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 1.547227\n",
            "Train Epoch: 2 [22720/60000 (38%)]\tLoss: 1.457468\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 1.708538\n",
            "Train Epoch: 2 [23360/60000 (39%)]\tLoss: 1.605934\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 1.687184\n",
            "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 1.679390\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 1.621637\n",
            "Train Epoch: 2 [24640/60000 (41%)]\tLoss: 1.445495\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 1.645682\n",
            "Train Epoch: 2 [25280/60000 (42%)]\tLoss: 1.615979\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.535150\n",
            "Train Epoch: 2 [25920/60000 (43%)]\tLoss: 1.667518\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 1.577764\n",
            "Train Epoch: 2 [26560/60000 (44%)]\tLoss: 1.456023\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 1.498986\n",
            "Train Epoch: 2 [27200/60000 (45%)]\tLoss: 1.734764\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 1.548362\n",
            "Train Epoch: 2 [27840/60000 (46%)]\tLoss: 1.623886\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 1.591176\n",
            "Train Epoch: 2 [28480/60000 (47%)]\tLoss: 1.596350\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 1.598671\n",
            "Train Epoch: 2 [29120/60000 (49%)]\tLoss: 1.628530\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 1.722325\n",
            "Train Epoch: 2 [29760/60000 (50%)]\tLoss: 1.584799\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 1.511399\n",
            "Train Epoch: 2 [30400/60000 (51%)]\tLoss: 1.536857\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 1.496981\n",
            "Train Epoch: 2 [31040/60000 (52%)]\tLoss: 1.442927\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 1.431112\n",
            "Train Epoch: 2 [31680/60000 (53%)]\tLoss: 1.549656\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.417067\n",
            "Train Epoch: 2 [32320/60000 (54%)]\tLoss: 1.519344\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 1.381453\n",
            "Train Epoch: 2 [32960/60000 (55%)]\tLoss: 1.571367\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 1.625692\n",
            "Train Epoch: 2 [33600/60000 (56%)]\tLoss: 1.408227\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 1.406484\n",
            "Train Epoch: 2 [34240/60000 (57%)]\tLoss: 1.523207\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 1.468675\n",
            "Train Epoch: 2 [34880/60000 (58%)]\tLoss: 1.401607\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 1.553344\n",
            "Train Epoch: 2 [35520/60000 (59%)]\tLoss: 1.399064\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 1.538556\n",
            "Train Epoch: 2 [36160/60000 (60%)]\tLoss: 1.600097\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 1.468042\n",
            "Train Epoch: 2 [36800/60000 (61%)]\tLoss: 1.554842\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 1.497443\n",
            "Train Epoch: 2 [37440/60000 (62%)]\tLoss: 1.558724\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 1.296349\n",
            "Train Epoch: 2 [38080/60000 (63%)]\tLoss: 1.345632\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.479442\n",
            "Train Epoch: 2 [38720/60000 (65%)]\tLoss: 1.397092\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 1.393455\n",
            "Train Epoch: 2 [39360/60000 (66%)]\tLoss: 1.477623\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 1.499592\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 1.523113\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 1.307700\n",
            "Train Epoch: 2 [40640/60000 (68%)]\tLoss: 1.363439\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 1.429566\n",
            "Train Epoch: 2 [41280/60000 (69%)]\tLoss: 1.533685\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 1.551103\n",
            "Train Epoch: 2 [41920/60000 (70%)]\tLoss: 1.488751\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 1.493386\n",
            "Train Epoch: 2 [42560/60000 (71%)]\tLoss: 1.358564\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 1.550912\n",
            "Train Epoch: 2 [43200/60000 (72%)]\tLoss: 1.406705\n",
            "Train Epoch: 2 [43520/60000 (73%)]\tLoss: 1.278349\n",
            "Train Epoch: 2 [43840/60000 (73%)]\tLoss: 1.482282\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 1.479692\n",
            "Train Epoch: 2 [44480/60000 (74%)]\tLoss: 1.274028\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.374035\n",
            "Train Epoch: 2 [45120/60000 (75%)]\tLoss: 1.355781\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 1.340390\n",
            "Train Epoch: 2 [45760/60000 (76%)]\tLoss: 1.332666\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 1.400678\n",
            "Train Epoch: 2 [46400/60000 (77%)]\tLoss: 1.270395\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 1.356831\n",
            "Train Epoch: 2 [47040/60000 (78%)]\tLoss: 1.238128\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 1.371379\n",
            "Train Epoch: 2 [47680/60000 (79%)]\tLoss: 1.225621\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 1.500308\n",
            "Train Epoch: 2 [48320/60000 (81%)]\tLoss: 1.274334\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 1.592026\n",
            "Train Epoch: 2 [48960/60000 (82%)]\tLoss: 1.313743\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 1.461077\n",
            "Train Epoch: 2 [49600/60000 (83%)]\tLoss: 1.228247\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 1.299164\n",
            "Train Epoch: 2 [50240/60000 (84%)]\tLoss: 1.403020\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 1.596164\n",
            "Train Epoch: 2 [50880/60000 (85%)]\tLoss: 1.390828\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.269741\n",
            "Train Epoch: 2 [51520/60000 (86%)]\tLoss: 1.423364\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 1.365933\n",
            "Train Epoch: 2 [52160/60000 (87%)]\tLoss: 1.338948\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 1.388185\n",
            "Train Epoch: 2 [52800/60000 (88%)]\tLoss: 1.369387\n",
            "Train Epoch: 2 [53120/60000 (89%)]\tLoss: 1.517549\n",
            "Train Epoch: 2 [53440/60000 (89%)]\tLoss: 1.298780\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 1.277513\n",
            "Train Epoch: 2 [54080/60000 (90%)]\tLoss: 1.333735\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 1.564119\n",
            "Train Epoch: 2 [54720/60000 (91%)]\tLoss: 1.439051\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 1.278645\n",
            "Train Epoch: 2 [55360/60000 (92%)]\tLoss: 1.374535\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 1.296462\n",
            "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 1.375739\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 1.595917\n",
            "Train Epoch: 2 [56640/60000 (94%)]\tLoss: 1.190791\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 1.369483\n",
            "Train Epoch: 2 [57280/60000 (95%)]\tLoss: 1.258626\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.308533\n",
            "Train Epoch: 2 [57920/60000 (97%)]\tLoss: 1.239180\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 1.252029\n",
            "Train Epoch: 2 [58560/60000 (98%)]\tLoss: 1.338972\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 1.402349\n",
            "Train Epoch: 2 [59200/60000 (99%)]\tLoss: 0.995130\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 1.205884\n",
            "Train Epoch: 2 [59840/60000 (100%)]\tLoss: 1.275006\n",
            "\n",
            "Test set: Average loss: 1.2358, Accuracy: 6615/10000 (66%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03f58390-36b9-43c7-93ae-63d8610509a9"
      },
      "source": [
        "c1_0 = model.start_weights[\"conv1\"]\n",
        "c1_1 = model.end_weights[\"conv1\"]\n"
      ],
      "id": "03f58390-36b9-43c7-93ae-63d8610509a9",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50aec0b2-db69-4acb-bc73-fad7c2ca8a07"
      },
      "source": [
        "#values_a = np.abs((c1_1 - c1_0).flatten())\n",
        "values_b = np.abs((c1_1 - c1_0).flatten())"
      ],
      "id": "50aec0b2-db69-4acb-bc73-fad7c2ca8a07",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM8DPykRRThf",
        "outputId": "57741b88-054b-460e-eba1-3e28d032dcd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "(values_a -values_b).sum()"
      ],
      "id": "rM8DPykRRThf",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9IIstkjG-rJ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from matplotlib.ticker import PercentFormatter"
      ],
      "id": "_9IIstkjG-rJ",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4__QX7uAL4_r",
        "outputId": "5c415ef1-e40a-45ac-a3ef-28371bd642ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        }
      },
      "source": [
        "n_bins = 100\n",
        "\n",
        "fig, axs = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
        "\n",
        "# We can set the number of bins with the `bins` kwarg\n",
        "axs.hist(values_a, bins=n_bins)\n"
      ],
      "id": "4__QX7uAL4_r",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([113.,  19.,   9.,   6.,   5.,   9.,   3.,   7.,  11.,   6.,   4.,\n",
              "          1.,   2.,   4.,   1.,   1.,   3.,   1.,   2.,   4.,   3.,   4.,\n",
              "          0.,   1.,   0.,   0.,   1.,   1.,   3.,   0.,   0.,   1.,   0.,\n",
              "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "          0.,   0.,   1.,   0.,   1.,   1.,   2.,   5.,   4.,   3.,   3.,\n",
              "          1.,   5.,   6.,   0.,   1.,   1.,   2.,   0.,   2.,   0.,   1.,\n",
              "          0.,   2.,   4.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   1.,\n",
              "          2.,   0.,   0.,   0.,   0.,   2.,   2.,   1.,   0.,   2.,   0.,\n",
              "          1.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   1.,   1.,   1.,\n",
              "          2.]),\n",
              " array([0.        , 0.00050827, 0.00101653, 0.0015248 , 0.00203307,\n",
              "        0.00254134, 0.0030496 , 0.00355787, 0.00406614, 0.00457441,\n",
              "        0.00508267, 0.00559094, 0.00609921, 0.00660747, 0.00711574,\n",
              "        0.00762401, 0.00813228, 0.00864054, 0.00914881, 0.00965708,\n",
              "        0.01016534, 0.01067361, 0.01118188, 0.01169015, 0.01219841,\n",
              "        0.01270668, 0.01321495, 0.01372322, 0.01423148, 0.01473975,\n",
              "        0.01524802, 0.01575628, 0.01626455, 0.01677282, 0.01728109,\n",
              "        0.01778935, 0.01829762, 0.01880589, 0.01931415, 0.01982242,\n",
              "        0.02033069, 0.02083896, 0.02134722, 0.02185549, 0.02236376,\n",
              "        0.02287203, 0.02338029, 0.02388856, 0.02439683, 0.02490509,\n",
              "        0.02541336, 0.02592163, 0.0264299 , 0.02693816, 0.02744643,\n",
              "        0.0279547 , 0.02846296, 0.02897123, 0.0294795 , 0.02998777,\n",
              "        0.03049603, 0.0310043 , 0.03151257, 0.03202083, 0.0325291 ,\n",
              "        0.03303737, 0.03354564, 0.0340539 , 0.03456217, 0.03507044,\n",
              "        0.03557871, 0.03608697, 0.03659524, 0.03710351, 0.03761178,\n",
              "        0.03812004, 0.03862831, 0.03913657, 0.03964484, 0.04015311,\n",
              "        0.04066138, 0.04116964, 0.04167791, 0.04218618, 0.04269445,\n",
              "        0.04320271, 0.04371098, 0.04421925, 0.04472752, 0.04523578,\n",
              "        0.04574405, 0.04625232, 0.04676058, 0.04726885, 0.04777712,\n",
              "        0.04828538, 0.04879365, 0.04930192, 0.04981019, 0.05031845,\n",
              "        0.05082672], dtype=float32),\n",
              " <a list of 100 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8SsDAV6M1Qy"
      },
      "source": [
        ""
      ],
      "id": "t8SsDAV6M1Qy",
      "execution_count": null,
      "outputs": []
    }
  ]
}